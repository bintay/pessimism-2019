{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.embeddings import WordEmbeddings, FlairEmbeddings, StackedEmbeddings\n",
    "from flair.data import Sentence\n",
    "import os\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATADIR = \"/Users/bent/Documents/programming/cl-twitter-personality/\"\n",
    "\n",
    "def loadData(filename, numberLines=-1, maxWords=-1):\n",
    "    data = {}\n",
    "    data[\"tweets\"] = []\n",
    "    data[\"sentiment\"] = []\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    file = open(os.path.join(DATADIR, filename), encoding=\"utf-8\")\n",
    "    minTokens = -1;\n",
    "    for lineNumber, line in enumerate(file):\n",
    "        # first line is headers\n",
    "        if (lineNumber == 0):\n",
    "            continue\n",
    "        \n",
    "        # only look at the first few lines\n",
    "        if (lineNumber == numberLines + 1):\n",
    "            break\n",
    "        \n",
    "        # one line per user\n",
    "        # columns[0] is index\n",
    "        # columns[1] is sentiment {0, 1}\n",
    "        # columns[2] Source\n",
    "        # columns[3] Tweet text\n",
    "        columns = line.split(\",\")\n",
    "        data[\"sentiment\"].append(int(columns[1]))\n",
    "        tokens = len(columns[3].split())\n",
    "        if (tokens < minTokens or minTokens == -1):\n",
    "            minTokens = tokens\n",
    "        if (maxWords != -1):\n",
    "            columns[3] = ' '.join(columns[3].split()[0:maxWords])\n",
    "        data[\"tweets\"].append(Sentence(columns[3]))\n",
    "        \n",
    "        if (lineNumber % 100000 == 0):\n",
    "            timeTaken = time.time() - start\n",
    "            timePerLine = timeTaken / (lineNumber + 1)\n",
    "            timeLeft = (numberLines - lineNumber - 1) * timePerLine\n",
    "            print(\"Line \" + str(lineNumber) + \" loaded. Time passed: \" + str(timeTaken) + \". Time left: \" + str(timeLeft) + \".\")\n",
    "    file.close()\n",
    "    print(\"Min number of tokens in data: \" + str(minTokens))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createEmbeddings(text, embeddings, numWords):\n",
    "    start = time.time()\n",
    "    embeddingList = []\n",
    "    for index, sentence in enumerate(text):\n",
    "        sentenceEmbeddings = []\n",
    "        embeddings.embed(sentence)\n",
    "        for token in sentence:\n",
    "            sentenceEmbeddings.append(token.embedding)\n",
    "        while (len(sentenceEmbeddings) < numWords):\n",
    "            sentenceEmbeddings.append(np.zeros(100))\n",
    "        embeddingList.append(sentenceEmbeddings)\n",
    "        if (index % 100000 == 0):\n",
    "            timeTaken = time.time() - start\n",
    "            timePerEmbedding = timeTaken / (index + 1)\n",
    "            timeLeft = (len(text) - index - 1) * timePerEmbedding\n",
    "            print(\"Embeddings: \" + str(index + 1) + \" / \" + str(len(text)) + \" complete. Time passed: \" + str(timeTaken) + \". Time left: \" + str(timeLeft) + \".\")\n",
    "        \n",
    "    return embeddingList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://stackoverflow.com/questions/4601373/better-way-to-shuffle-two-numpy-arrays-in-unison\n",
    "def shuffle_in_unison(a, b):\n",
    "    rng_state = np.random.get_state()\n",
    "    np.random.shuffle(a)\n",
    "    np.random.set_state(rng_state)\n",
    "    np.random.shuffle(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embeddings = StackedEmbeddings([\n",
    "#                                WordEmbeddings('glove'), \n",
    "#                                FlairEmbeddings('news-forward'), \n",
    "#                                FlairEmbeddings('news-backward'),\n",
    "#                              ])\n",
    "embeddings = WordEmbeddings('glove')\n",
    "\n",
    "numberWords = 35\n",
    "numberLines = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data\n",
      "Min number of tokens in data: 2\n",
      "Data loaded\n",
      "Creating embeddings\n",
      "Embeddings: 1 / 100 complete. Time passed: 0.0006239414215087891. Time left: 0.06177020072937012.\n",
      "Embeddings created\n",
      "0\n",
      "Sentence: \"is so sad for my APL friend.............\" - 7 Tokens\n"
     ]
    }
   ],
   "source": [
    "print('Loading data')\n",
    "data = loadData('SentimentAnalysisDataset.csv', numberLines, numberWords)\n",
    "print('Data loaded')\n",
    "\n",
    "# shuffle the data\n",
    "#shuffle_in_unison(data[\"sentiment\"], data[\"tweets\"])\n",
    "\n",
    "# create the embeddings\n",
    "print('Creating embeddings')\n",
    "data[\"embeddings\"] = createEmbeddings(data[\"tweets\"], embeddings, numberWords)\n",
    "print('Embeddings created')\n",
    "\n",
    "# print one piece of data to make sure everything is good\n",
    "print(data[\"sentiment\"][0])\n",
    "print(data[\"tweets\"][0])\n",
    "#print(data[\"embeddings\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tranforming embeddings into numpy array\n",
      "Done transforming embeddings\n",
      "Transforming data into numpy arrays\n",
      "Done transforming data\n"
     ]
    }
   ],
   "source": [
    "# turn embedding list and personality list into numpy array\n",
    "\n",
    "print(\"Tranforming embeddings into numpy array\")\n",
    "for index, embeddings in enumerate(data[\"embeddings\"]):\n",
    "    for index2, embedding in enumerate(embeddings):\n",
    "        # change PyTorch tensor to NumPy array\n",
    "        try:\n",
    "            data[\"embeddings\"][index][index2] = embedding.numpy()\n",
    "        except:\n",
    "            pass\n",
    "print(\"Done transforming embeddings\")\n",
    "\n",
    "embeddingDim = len(data[\"embeddings\"][0][0]);\n",
    "\n",
    "print(\"Transforming data into numpy arrays\")\n",
    "data[\"embeddings\"] = np.array(data[\"embeddings\"])\n",
    "\n",
    "data[\"embeddings\"] = np.resize(data[\"embeddings\"], (len(data[\"embeddings\"]), numberWords, embeddingDim, 1))\n",
    "\n",
    "data[\"sentiment\"] = np.array(data[\"sentiment\"])\n",
    "print(\"Done transforming data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 35, 100, 1) (100,)\n",
      "Word vector size: 100\n",
      "Number of tweets: 100\n",
      "Number of words per  tweet: 35\n"
     ]
    }
   ],
   "source": [
    "X = data[\"embeddings\"]\n",
    "y = data[\"sentiment\"]\n",
    "\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "print(\"Word vector size: \" + str(X.shape[2]))\n",
    "print(\"Number of tweets: \" + str(X.shape[0]))\n",
    "print(\"Number of words per  tweet: \" + str(X.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the model\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D, Concatenate, Average, Input\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(64, (3,100), input_shape=X.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,1)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 90 samples, validate on 10 samples\n",
      "Epoch 1/10\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 0.6514 - acc: 0.7556 - val_loss: 0.5315 - val_acc: 0.8000\n",
      "Epoch 2/10\n",
      "90/90 [==============================] - 0s 184us/step - loss: 0.5933 - acc: 0.7444 - val_loss: 0.5144 - val_acc: 0.8000\n",
      "Epoch 3/10\n",
      "90/90 [==============================] - 0s 211us/step - loss: 0.5461 - acc: 0.7444 - val_loss: 0.5097 - val_acc: 0.8000\n",
      "Epoch 4/10\n",
      "90/90 [==============================] - 0s 204us/step - loss: 0.5141 - acc: 0.7444 - val_loss: 0.5109 - val_acc: 0.8000\n",
      "Epoch 5/10\n",
      "90/90 [==============================] - 0s 208us/step - loss: 0.4797 - acc: 0.7444 - val_loss: 0.5063 - val_acc: 0.8000\n",
      "Epoch 6/10\n",
      "90/90 [==============================] - 0s 206us/step - loss: 0.4411 - acc: 0.7444 - val_loss: 0.5033 - val_acc: 0.8000\n",
      "Epoch 7/10\n",
      "90/90 [==============================] - 0s 246us/step - loss: 0.4008 - acc: 0.7667 - val_loss: 0.5090 - val_acc: 0.8000\n",
      "Epoch 8/10\n",
      "90/90 [==============================] - 0s 202us/step - loss: 0.3560 - acc: 0.8111 - val_loss: 0.5250 - val_acc: 0.7000\n",
      "Epoch 9/10\n",
      "90/90 [==============================] - 0s 201us/step - loss: 0.3126 - acc: 0.8333 - val_loss: 0.5431 - val_acc: 0.7000\n",
      "Epoch 10/10\n",
      "90/90 [==============================] - 0s 205us/step - loss: 0.2774 - acc: 0.8889 - val_loss: 0.5782 - val_acc: 0.7000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x147c64e80>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, batch_size=32, validation_split=0.1, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Proportion: 0.669\n",
      "1 Proportion: 0.331\n"
     ]
    }
   ],
   "source": [
    "numI = 0\n",
    "numE = 0\n",
    "for i in range(0, len(y)):\n",
    "    if (y[i] == 0):\n",
    "        numI = numI + 1\n",
    "    elif (y[i] == 1):\n",
    "        numE = numE + 1\n",
    "\n",
    "print(\"0 Proportion: \" + str(numI / len(y)))\n",
    "print(\"1 Proportion: \" + str(numE / len(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-0.15251 ]\n",
      "  [ 0.14106 ]\n",
      "  [ 0.62195 ]\n",
      "  ...\n",
      "  [ 0.36824 ]\n",
      "  [ 0.28545 ]\n",
      "  [-0.58772 ]]\n",
      "\n",
      " [[-0.077053]\n",
      "  [ 1.5622  ]\n",
      "  [ 0.69068 ]\n",
      "  ...\n",
      "  [-0.32393 ]\n",
      "  [ 0.37064 ]\n",
      "  [-1.3264  ]]\n",
      "\n",
      " [[-0.2857  ]\n",
      "  [ 0.3816  ]\n",
      "  [ 0.55507 ]\n",
      "  ...\n",
      "  [-0.06151 ]\n",
      "  [-0.090062]\n",
      "  [ 0.5536  ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-0.33979 ]\n",
      "  [ 0.20941 ]\n",
      "  [ 0.46348 ]\n",
      "  ...\n",
      "  [-0.23394 ]\n",
      "  [ 0.47298 ]\n",
      "  [-0.028803]]\n",
      "\n",
      " [[ 0.      ]\n",
      "  [ 0.      ]\n",
      "  [ 0.      ]\n",
      "  ...\n",
      "  [ 0.      ]\n",
      "  [ 0.      ]\n",
      "  [ 0.      ]]\n",
      "\n",
      " [[ 0.      ]\n",
      "  [ 0.      ]\n",
      "  [ 0.      ]\n",
      "  ...\n",
      "  [ 0.      ]\n",
      "  [ 0.      ]\n",
      "  [ 0.      ]]]\n"
     ]
    }
   ],
   "source": [
    "print(data['embeddings'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
